<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Measures of Location and Dispersion — In-depth Survey and Practical Guidance</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root{ --bg:#0f111a; --panel:#1a1c2b; --accent:#1976D2; --muted:#cfd8cf; --text:#e6f6ff }
    *{box-sizing:border-box}
    html,body{height:100%;margin:0;font-family:Inter, 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;background:var(--bg);color:var(--text);line-height:1.7}
    .container{max-width:1100px;margin:0 auto;padding:2rem}
    a{color:var(--accent)}
    .topnav{display:flex;gap:0.75rem;flex-wrap:wrap;margin-bottom:1rem}
    .card{background:var(--panel);border-left:5px solid var(--accent);padding:1.25rem;border-radius:8px;margin:1rem 0;box-shadow:0 8px 24px rgba(0,0,0,0.6)}
    h1,h2,h3{color:var(--accent);margin:0}
    h1{font-size:2rem;padding-bottom:0.35rem;border-bottom:3px solid rgba(25,118,210,0.12);margin-bottom:1rem}
    p{max-width:90ch;text-align:justify}
    .lead{color:#eaf6ff;font-size:1.02rem}
    .display-equation{background:rgba(255,255,255,0.02);padding:12px;border-radius:6px;border:1px solid rgba(255,255,255,0.03);margin:12px 0}
    table{width:100%;border-collapse:collapse;margin-top:8px}
    th,td{padding:8px;border:1px solid rgba(255,255,255,0.06);text-align:left}
    th{background:rgba(25,118,210,0.06)}
    pre{background:#0b0c10;padding:12px;border-radius:6px;overflow:auto;color:#dfeffb}
    code{font-family:monospace;background:rgba(255,255,255,0.02);padding:2px 6px;border-radius:4px}
    ul{max-width:90ch}
    footer{color:var(--muted);text-align:center;margin-top:18px}
    .muted{color:var(--muted)}
    @media (max-width:640px){.topnav{font-size:0.95rem}}
  </style>

  <script>
    window.MathJax = {
      loader: { load: ['input/tex', 'output/chtml'] },
      tex: { packages: {'[+]': ['ams','amsmath','amssymb']}, inlineMath: [['\\(','\\)'], ['$', '$']], displayMath: [['\\[','\\]']] },
      chtml: { scale: 1.05 },
      startup: { typeset: true }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
  <div class="container">
    <header class="topnav">
      <a href="#intro">Introduction</a>
      <a href="#location">Measures of Location</a>
      <a href="#dispersion">Measures of Dispersion</a>
      <a href="#robustness">Robustness & IF</a>
      <a href="#comparison">Comparison & Guidance</a>
      <a href="#examples">Examples</a>
      <a href="#code">Code</a>
      <a href="#refs">References</a>
    </header>

    <main>
      <article class="card" id="intro">
        <h1>Measures of Location and Dispersion — In-depth Survey and Practical Guidance</h1>
      </article>

      <article class="card" id="location">
        <h2>1. Measures of location (central tendency)</h2>

        <section>
          <h3>1.1 Arithmetic mean</h3>
          <p><strong>Definition.</strong> For observations $x_1,\dots,x_n$:</p>
          <div class="display-equation">\[ \n            \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i.
          \]</div>

          <p><strong>Loss interpretation.</strong> The mean minimizes quadratic loss:</p>
          <div class="display-equation">\[ \n            \bar{x} = \arg\min_m \sum_{i=1}^n (x_i - m)^2.
          \]</div>

          <p><strong>Sampling properties (classical).</strong> Under i.i.d. sampling from a distribution with finite variance $\sigma^2$,</p>
          <div class="display-equation">\[ \n            \mathbb{E}[\bar{X}] = \mu, \qquad \mathrm{Var}(\bar{X}) = \frac{\sigma^2}{n}.
          \]</div>

          <p>By the central limit theorem (CLT), $\sqrt{n}(\bar{X}-\mu)\xrightarrow{d} N(0,\sigma^2)$ under mild conditions, justifying approximate Gaussian inference for large $n$.</p>

          <p><strong>Advantages.</strong> Efficient under Gaussian errors (attains Cramér–Rao lower bound for location in normal model), additive, simple estimation and inferential apparatus (SE, t-tests, confidence intervals).</p>

          <p><strong>Limitations.</strong> Highly sensitive to outliers and heavy tails; when $\mathrm{Var}(X)=\infty$ the sample mean does not concentrate and inference breaks down. For skewed or contaminated data, other estimators may be preferable.</p>
        </section>

        <section>
          <h3>1.2 Median</h3>
          <p><strong>Definition.</strong> Let $x_{(1)}\le\dots\le x_{(n)}$ be the order statistics. The sample median $\tilde{x}$ is the middle value (or average of the two middle values if $n$ is even).</p>

          <p><strong>Loss interpretation.</strong> Minimizes absolute deviations:</p>
          <div class="display-equation">\[ \n            \tilde{x} = \arg\min_m \sum_{i=1}^n |x_i - m|.
          \]</div>

          <p><strong>Asymptotic variance.</strong> If the underlying density $f$ at the true median $m$ is positive, then asymptotically</p>
          <div class="display-equation">\[ \n            \sqrt{n}(\tilde{X}-m) \xrightarrow{d} N\left(0,\frac{1}{4f(m)^2}\right).
          \]</div>

          <p>This shows the median's efficiency depends on the density at the center; under normality it is less efficient than the mean (asymptotic relative efficiency ~0.64).</p>

          <p><strong>Advantages.</strong> Robust (breakdown point ≈ 50%), bounded influence function, natural for skewed data or ordinal scales.</p>
        </section>

        <section>
          <h3>1.3 Mode</h3>
          <p><strong>Definition.</strong> The mode is the value (or set of values) with highest probability mass (discrete case) or highest density (continuous case). For continuous data the sample mode can be estimated via kernel density estimation or histogram peaks.</p>

          <p><strong>Use cases.</strong> Categorical/nominal variables, multimodal distributions to reveal subpopulations. Not generally used as a sole location estimator for continuous unimodal symmetric data.</p>
        </section>

        <section>
          <h3>1.4 Geometric mean and harmonic mean</h3>
          <p><strong>Geometric mean.</strong> For strictly positive data:</p>
          <div class="display-equation">\[ \n            G = \left(\prod_{i=1}^n x_i\right)^{1/n} = \exp\left(\frac{1}{n}\sum_{i=1}^n \ln x_i\right).
          \]</div>
          <p>Appropriate for multiplicative processes (compounded returns) and ratios; stable under log-transform.</p>

          <p><strong>Harmonic mean.</strong> For positive rates $x_i$:</p>
          <div class="display-equation">\[ \n            H = \left(\frac{1}{n}\sum_{i=1}^n \frac{1}{x_i}\right)^{-1}.
          \]</div>
          <p>Useful when averaging rates where the denominator is fixed (e.g., average speed over equal distances). Extremely sensitive to small values and zeros.</p>
        </section>

        <section>
          <h3>1.5 Power means and Lehmer means</h3>
          <div class="display-equation">\[ \n            M_r(x) = \left(\frac{1}{n}\sum_{i=1}^n x_i^r\right)^{1/r}, \quad L_p(x) = \frac{\sum x_i^p}{\sum x_i^{p-1}}.
          \]</div>
          <p>These families interpolate among common means: $M_1$ is arithmetic, $M_0$ (limit) is geometric, $M_{-1}$ harmonic, $M_2$ RMS. Monotonicity in the parameter controls tail emphasis.</p>

        </section>

        <section>
          <h3>1.6 Trimmed, Winsorized means and M-estimators</h3>
          <p><strong>Trimmed mean.</strong> Remove an equal percentage $\alpha$ from each tail and compute the mean of the remaining data. This reduces influence of extremes while maintaining interpretability.</p>

          <p><strong>Winsorized mean.</strong> Replace the extreme $\alpha$ fraction by the corresponding percentile values, then compute the mean — preserves sample size and reduces variance.</p>

          <p><strong>M-estimators (robust).</strong> Solve for $\hat\theta$:</p>
          <div class="display-equation">\[ \n            \sum_{i=1}^n \psi\left(\frac{x_i-\hat\theta}{s}\right) = 0,
          \]</div>
          <p>where $\psi$ is the derivative of a chosen loss $\rho$, and $s$ is a robust scale (MAD, etc.). Examples: Huber's $\psi$ (combines quadratic near 0 and linear tails), Tukey's biweight (redescending $\psi$ that completely rejects extreme points).</p>

          <p>M-estimators balance efficiency and robustness; tuning constants control trade-offs.</p>
        </section>
      </article>

      <article class="card" id="dispersion">
        <h2>2. Measures of dispersion (spread)</h2>

        <section>
          <h3>2.1 Variance and standard deviation</h3>
          <p><strong>Definition (sample variance, unbiased):</strong></p>
          <div class="display-equation">\[ \n            s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2, \qquad s = \sqrt{s^2}.
          \]</div>

          <p>Variance is central to many inferential procedures because of additivity for independent variables and its relationship to quadratic loss. Standard deviation $s$ is in the same units as the data and is used extensively for reporting volatility and constructing parametric confidence intervals.</p>

          <p><strong>Notes.</strong> For heavy-tailed distributions variance may be infinite or dominated by a few observations; consider robust alternatives in such cases.</p>
        </section>

        <section>
          <h3>2.2 Interquartile range (IQR)</h3>
          <p><strong>Definition.</strong> $\mathrm{IQR} = Q_3 - Q_1$ where $Q_1$ and $Q_3$ are the 25th and 75th percentiles.</p>
          <p>IQR is robust to outliers and useful for boxplots and robust spread summaries. Often used with rule-of-thumb outlier detection: a point is an outlier if it lies outside $[Q_1 - 1.5\mathrm{IQR}, Q_3 + 1.5\mathrm{IQR}]$.</p>
        </section>

        <section>
          <h3>2.3 Median absolute deviation (MAD)</h3>
          <p><strong>Definition (median-based):</strong></p>
          <div class="display-equation">\[ \n            \mathrm{MAD} = \mathrm{median}_i\left(|x_i - \mathrm{median}(x)|\right).
          \]</div>
          <p>MAD is a highly robust scale estimator (breakdown point 50%). Often scaled by a constant (\(1/\Phi^{-1}(3/4)\) ≈ 1.4826 for normal consistency) to estimate $\sigma$ under normality.</p>
        </section>

        <section>
          <h3>2.4 Coefficient of variation (CV)</h3>
          <div class="display-equation">\[ \n            \mathrm{CV} = \frac{s}{\bar{x}}.
          \]</div>
          <p>CV is dimensionless and useful for comparing variability across datasets with different means/units. Not meaningful when the mean is near zero, and unstable for heavy-tailed data.</p>
        </section>

        <section>
          <h3>2.5 Other measures</h3>
          <ul>
            <li><strong>Range:</strong> $\max - \min$. Simple but entirely non-robust.</li>
            <li><strong>Percentile-based spreads:</strong> e.g. 10–90% range, trimmed standard deviation.</li>
            <li><strong>Quantile regression residual scales:</strong> model-based dispersion measures robust to asymmetric noise.</li>
          </ul>
        </section>
      </article>

      <article class="card" id="robustness">
        <h2>3. Robustness concepts: breakdown point, influence function, and efficiency</h2>

        <section>
          <h3>3.1 Breakdown point</h3>
          <p>Informally, the finite-sample breakdown point of an estimator is the smallest fraction of observations that needs to be replaced by arbitrary values to make the estimator arbitrarily bad (e.g., unbounded). For a sample of size $n$:</p>
          <ul>
            <li>Sample mean: breakdown point $=1/n$ (practically 0 for large $n$).</li>
            <li>Sample median: breakdown point $\approx 0.5$ (robust).</li>
            <li>Trimmed means: breakdown point equals trimming proportion.</li>
          </ul>
        </section>

        <section>
          <h3>3.2 Influence function (IF)</h3>
          <p>For an estimator $T$ defined at distribution $F$, the influence function at point $x$ is</p>
          <div class="display-equation">\[ \n            \mathrm{IF}(x;T,F) = \lim_{\varepsilon\downarrow 0} \frac{T((1-\varepsilon)F + \varepsilon\delta_x)-T(F)}{\varepsilon},
          \]</div>
          <p>which quantifies the effect of an infinitesimal contamination at $x$. If the IF is bounded then single extreme observations cannot exert arbitrarily large effect.</p>

          <p><strong>Examples.</strong></p>
          <ul>
            <li><strong>Mean:</strong> $\mathrm{IF}(x;\mathrm{mean},F)=x-\mu$ (unbounded → non-robust).</li>
            <li><strong>Median:</strong> $\mathrm{IF}(x;\mathrm{median},F) = \dfrac{\operatorname{sgn}(x-m)}{2f(m)}$ (bounded if $f(m)>0$).</li>
            <li><strong>Huber M-estimator:</strong> IF is bounded by the tuning constant, providing controlled sensitivity.</li>
          </ul>
        </section>

        <section>
          <h3>3.3 Efficiency and influence on variance</h3>
          <p>An estimator's asymptotic variance can be expressed using the IF: for large $n$,</p>
          <div class="display-equation">\[ \n            \mathrm{Var}(\sqrt{n}(T_n - T)) \approx \mathbb{E}_F[\mathrm{IF}(X;T,F)^2].
          \]</div>
          <p>Thus choosing an estimator with a smaller squared-IF under a model increases asymptotic efficiency. The efficiency vs robustness trade-off is central: highly robust estimators often sacrifice some efficiency under the ideal model.</p>
        </section>
      </article>

      <article class="card" id="comparison">
        <h2>4. Practical comparison and selection guidelines</h2>

        <table>
          <thead><tr><th>Estimator</th><th>Strengths</th><th>Weaknesses/When to avoid</th></tr></thead>
          <tbody>
            <tr><td>Mean</td><td>Efficient under normal, simple inference</td><td>Sensitive to outliers, heavy tails</td></tr>
            <tr><td>Median</td><td>Robust, good for skewed data</td><td>Less efficient under normal (< --- approx 64% eff.)</td></tr>
            <tr><td>Trimmed mean (e.g., 10–20%)</td><td>Compromise: robust and relatively efficient</td><td>Choice of trim fraction is ad hoc</td></tr>
            <tr><td>M-estimators (Huber)</td><td>Tunable balance: near-efficient under normal and robust against outliers</td><td>Requires scale estimate and tuning</td></tr>
            <tr><td>Geometric mean</td><td>Proper for multiplicative processes</td><td>Requires positive data; sensitive to zeros</td></tr>
            <tr><td>IQR, MAD</td><td>Robust measures of spread</td><td>Less efficient if data are truly normal and outlier-free</td></tr>
          </tbody>
        </table>

        <p><strong>Practical heuristics:</strong></p>
        <ol>
          <li>If the sample looks symmetric and there are no obvious gross errors → mean & standard deviation.</li>
          <li>If data are skewed / contain outliers → median & IQR or robust M-estimator and MAD.</li>
          <li>When comparing volatility across different mean levels → use coefficient of variation (with caution when mean≈0).</li>
          <li>For compounded growth or multiplicative processes → geometric mean.</li>
        </ol>
      </article>

      <article class="card" id="examples">
        <h2>5. Worked numerical examples and diagnostics</h2>

        <h3>5.1 Outlier sensitivity</h3>
        <p>Data: 10, 12, 11, 13, 100.</p>
        <p>Compute central tendencies:</p>
        <ul>
          <li>Mean $= (10+12+11+13+100)/5 = 29.2$</li>
          <li>Median $=12$</li>
          <li>Trimmed mean 20% (drop min and max) → mean(12,11,13)=12</li>
        </ul>
        <p>Observation: mean inflated by outlier; robust measures (median, trimmed) reflect central cluster.</p>

        <h3>5.2 Variance vs IQR</h3>
        <p>Same data: compute sample variance (unbiased):</p>
        <div class="display-equation">\[ \n          s^2 = \frac{1}{4}\sum (x_i-29.2)^2 \approx 1508.2, \quad s\approx 38.84.
        \]</div>
        <p>IQR computed from order statistics {10,11,12,13,100}: $Q_1=11$, $Q_3=13$, IQR=2 — which shows the bulk of the data is tightly clustered despite large variance driven by outlier.</p>

        <h3>5.3 Robust scale: MAD</h3>
        <p>MAD = median(|x - median(x)|) = median(|[2,1,0,1,88]|) = 1. Scaling for normal consistency: $1.4826\times1 = 1.4826$ (robust estimate of $\sigma$).</p>
      </article>

      <article class="card" id="code">
        <h2>6. Practical code snippets</h2>
        <p>Below are small examples that compute common measures. These are self-contained and are useful for reproducing results in practice.</p>

        <h3>6.1 Python (NumPy / SciPy / statsmodels)</h3>
        <pre><code>import numpy as np
from scipy import stats
x = np.array([10,12,11,13,100])
print('mean', x.mean())
print('median', np.median(x))
print('trimmed mean (20%)', stats.trim_mean(x, 0.2))
print('sample variance (unbiased)', x.var(ddof=1))
print('IQR', stats.iqr(x))
print('MAD (median-based)', np.median(np.abs(x - np.median(x))))

# Huber location via statsmodels
from statsmodels import robust
print('Huber location', robust.scale.huber(x))
</code></pre>

        <h3>6.2 R</h3>
        <pre><code># Basic R code
x <- c(10,12,11,13,100)
mean(x)
median(x)
# trimmed mean (10% each side)
mean(x, trim=0.1)
var(x) # sample variance uses n-1 by default
IQR(x)
mad(x, constant=1.4826) # median absolute deviation (scaled)

# robustbase and MASS provide robust estimators
library(robustbase)
hubers <- huber(x)
</code></pre>

        <h3>6.3 JavaScript (client-side quick calculator)</h3>
        <pre><code>function stats(arr){
  arr = arr.slice().sort((a,b)=>a-b);
  const n = arr.length;
  const mean = arr.reduce((a,b)=>a+b,0)/n;
  const median = n%2 ? arr[(n-1)/2] : (arr[n/2-1]+arr[n/2])/2;
  const diffs = arr.map(v=>Math.abs(v-mean));
  const variance = arr.reduce((s,v)=>s+(v-mean)*(v-mean),0)/(n-1);
  return {mean,median,variance,sd:Math.sqrt(variance)};
}
</code></pre>
      </article>

      <article class="card" id="refs">
        <h2>7. References and further reading</h2>
        <ol>
          <li>P. J. Huber and E. M. Ronchetti, <em>Robust Statistics</em>, 2nd ed., Wiley, 2009 — modern overview of M-estimation and robustness theory.</li>
          <li>R. A. Maronna, R. D. Martin, V. J. Yohai, <em>Robust Statistics: Theory and Methods</em>, Wiley, 2006.</li>
          <li>J. W. Tukey, <em>Exploratory Data Analysis</em>, Addison-Wesley, 1977 — classic for robust exploratory summaries.</li>
          <li>M. H. DeGroot and M. J. Schervish, <em>Probability and Statistics</em>, 4th ed. — sampling theory and estimator properties.</li>
          <li>Hampel, Ronchetti, Rousseeuw, and Stahel, <em>Robust Statistics: The Approach Based on Influence Functions</em> — foundational for IF and breakdown point.</li>
        </ol>

        <p class="muted">Authors: prepared as an educational resource. If you want, I can add citations to specific theorems and papers (with DOIs), interactive calculators embedded in the page, or export this HTML as a downloadable file.</p>
      </article>

    </main>

    <footer class="muted">© 2025 — Comprehensive survey on measures of location and dispersion</footer>
  </div>

  <script>
    if (window.MathJax && MathJax.typesetPromise) MathJax.typesetPromise().catch(()=>{});
  </script>
</body>
</html>
